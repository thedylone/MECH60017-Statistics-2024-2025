---
title: "Probability with vanilla R"
author: "Dylan Chua"
date: "November 2024"
output:
  html_document:
    self_contained: yes
---

```{r setup-chunk, include=FALSE}
knitr::opts_chunk$set(
  dev = "png",
  dpi = 300,
  cache = FALSE,
  fig.path = "plots/R/"
)
```

# 1 Simulation of the probability that athletes took drugs given a positive test result

In this exercise you will simulate the conditional probability that an athlete took drugs, given he/she failed a drug test.

Scenario: Suppose you have a population of 100 athletes. It is known that 10% of athletes take drugs. There is a test that will correctly identify 80% of drug users and also incorrectly identify 20% of clean athletes as drug users.

Simulate this situation: Each athlete in population of size 100 has probability 0.1 of being a drug taker. For each simulated athlete, generate a random integer 1-10 - if this is 1 the simulated athlete takes drugs, if this is 2-10 does not take drugs. 80% of the population is correctly identified by the test - for each simulated athlete, generate a second random integer 1-10 - if this is 1 or 2 the simulated athlete's test is incorrect, otherwise correct.

Athletes who fail the test include drug takers correctly identified as cheats and clean athletes incorrectly identified as cheats. So a simulated athlete is deemed to have failed if his/her 1st random integer is 1 and 2nd random integer is 3-10 OR 1st integer 2-10 and 2nd integer 1-2.

You are now in a position to calculate the first simulated conditional probability of drug taking among the athletes who failed the test.

```{r simulation}
simulation <- function(
    n = 100,
    p_drug = 0.1,
    p_true_pos = 0.8,
    p_false_pos = 0.2,
    two_tests = FALSE) {
  # population of athletes
  drug <- runif(n) < p_drug
  # test results
  test <- runif(n)

  # simulate drug test
  # if drug taker and correctly identified or clean and incorrectly identified
  if (two_tests) {
    second_test <- runif(n)
    result <- (drug & test <= p_true_pos & second_test <= p_true_pos) |
      (!drug & test <= p_false_pos & second_test <= p_false_pos)
  } else {
    result <- (drug & test <= p_true_pos) | (!drug & test <= p_false_pos)
  }

  # calculate conditional probability of drug taking given failed test
  p_drug_fail <- sum(drug & result) / sum(result)

  return(p_drug_fail)
}

simulation()

plot_density <- function(simulations, main = "P(takes drugs | failed test)") {
  hist(simulations,
    main = main,
    freq = FALSE,
    xlab = "Probability",
    col = "skyblue",
    border = "white",
    xlim = c(0, 1)
  )
  lines(density(simulations),
    lwd = 2,
    col = "red"
  )
}
```

### 1. Repeat this simulation many times (for example, 1000 times). Plot (histogram or density) the resulting simulated probabilities. What would you consider to be an unusual result?

```{r repeat-simulation}
# repeat simulation 1000 times
simulations <- replicate(1000, simulation())

# plot histogram of simulated probabilities
plot_density(simulations)
```

### 2. Try changing the assumptions/ parameters of the model - for instance, change the rate of drug taking in the population of athletes, the proportion of drug takers correctly identified by the test, the proportion of clean athletes incorrectly identified (in this example these last two proportions are equal, but this would usually not be the case).

```{r change-p-drug}
# probability of drug taking in the population of athletes increased to 20%
p_drug_simulations <- replicate(1000, simulation(p_drug = 0.2))

# plot histogram of simulated probabilities
plot_density(p_drug_simulations,
  main = "P(takes drugs | failed test) (20% drug takers)"
)
```

```{r change-p-true-pos}
# proportion of drug takers correctly identified by the test increased to 90%
p_true_pos_simulations <- replicate(1000, simulation(p_true_pos = 0.9))

# plot histogram of simulated probabilities
plot_density(p_true_pos_simulations,
  main = "P(takes drugs | failed test) (90% true positive rate)"
)
```

```{r change-p-false-pos}
# false positive rate increased to 10%
p_false_pos_simulations <- replicate(1000, simulation(p_false_pos = 0.1))

# plot histogram of simulated probabilities
plot_density(p_false_pos_simulations,
  main = "P(takes drugs | failed test) (10% false positive rate)"
)
```

```{r compare-simulations}
# compare the simulations
par(mfrow = c(2, 2))
plot_density(simulations, main = "P(takes drugs | failed test)")
plot_density(p_drug_simulations,
  main = "P(takes drugs | failed test) (20% drug takers)"
)
plot_density(p_true_pos_simulations,
  main = "P(takes drugs | failed test) (90% true positive rate)"
)
plot_density(p_false_pos_simulations,
  main = "P(takes drugs | failed test) (10% false positive rate)"
)
```

### 3. Now simulate the situation where the athlete is only deemed to have failed if he/she has failed two tests, where the tests are conditionally independent given the athlete's drug-taking status.

```{r two-tests}
# repeat simulation 1000 times
simulations_two_tests <- replicate(1000, simulation(two_tests = TRUE))

# plot histogram of simulated probabilities
par(mfrow = c(1, 1))
plot_density(simulations_two_tests,
  main = "P(takes drugs | failed test) (two tests)"
)
```

### 4. Can you suggest ways to improve the simulation (e.g. write code efficiently)?

### 5. Use Bayes' Theorem to calculate the conditional probability of drug taking given failing (a) one test (b) two tests that are conditionally independent given drug-taking status. (Use the probabilites as specified in the original scenario.) Does this agree with your simulation?

```{r bayes-theorem}
# Bayes' Theorem
# P(takes drugs | failed test) = P(takes drugs and failed test) / P(failed test)

p_drug <- 0.1
p_true_pos <- 0.8
p_false_pos <- 0.2

print(paste(
  "P(takes drugs | failed test): ",
  p_drug * p_true_pos / (p_drug * p_true_pos + (1 - p_drug) * p_false_pos)
))
```

# 2 Probability Distributions

Initially choose two distributions (one discrete and one continuous).

Selected: Poisson and Gamma distributions.

### 1. Plot the p.m.f. or p.d.f. and c.d.f. for a suitable choice of parameters for each distribution. You decide the range of values and parameter(s) for each distribution.

```{r plot-distributions}
points <- seq(0, 100, by = 1)
pdfs <- list()
cdfs <- list()

# Binomial distribution
N <- 100
p <- 0.25
binomial_pmf <- dbinom(points, size = N, prob = p)
pdfs$Binomial <- binomial_pmf
binomial_cdf <- pbinom(points, size = N, prob = p)
cdfs$Binomial <- binomial_cdf

# Geometric distribution
p <- 0.25
geometric_pmf <- dgeom(points, prob = p)
pdfs$Geometric <- geometric_pmf
geometric_cdf <- pgeom(points, prob = p)
cdfs$Geometric <- geometric_cdf

# Poisson distribution
lambda <- 25
poisson_pmf <- dpois(points, lambda = lambda)
pdfs$Poisson <- poisson_pmf
poisson_cdf <- ppois(points, lambda = lambda)
cdfs$Poisson <- poisson_cdf

# Continuous uniform distribution
uniform_pdf <- dunif(points, min = 0, max = 1)
pdfs$Uniform <- uniform_pdf
uniform_cdf <- punif(points, min = 0, max = 1)
cdfs$Uniform <- uniform_cdf

# Exponential distribution
rate <- 1
exponential_pdf <- dexp(points, rate = rate)
pdfs$Exponential <- exponential_pdf
exponential_cdf <- pexp(points, rate = rate)
cdfs$Exponential <- exponential_cdf

# Normal distribution
mean <- 25
sd <- 5
normal_pdf <- dnorm(points, mean = mean, sd = sd)
pdfs$Normal <- normal_pdf
normal_cdf <- pnorm(points, mean = mean, sd = sd)
cdfs$Normal <- normal_cdf

# Student's t distribution
df <- 25
t_pdf <- dt(points, df = df)
pdfs$Student <- t_pdf
t_cdf <- pt(points, df = df)
cdfs$Student <- t_cdf

# Gamma distribution
shape <- 25
scale <- 1
gamma_pdf <- dgamma(points, shape = shape, scale = scale)
pdfs$Gamma <- gamma_pdf
gamma_cdf <- pgamma(points, shape = shape, scale = scale)
cdfs$Gamma <- gamma_cdf

plot_pdf_cdf <- function(points, pdfs, cdfs) {
  n <- length(pdfs)
  col_pal <- rainbow(n)
  # par(mfcol = c(2 * ceiling(sqrt(n)), ceiling(sqrt(n))))
  par(mfrow = c(1, 2))

  for (i in 1:n) {
    plot(points, pdfs[[i]],
      lwd = 2,
      type = "h",
      xlab = "x",
      ylab = "Density",
      main = paste(names(pdfs)[i], "PDF"),
      col = col_pal[i]
    )
    plot(points, cdfs[[i]],
      lwd = 2,
      type = "h",
      xlab = "x",
      ylab = "Cumulative density",
      main = paste(names(cdfs)[i], "CDF"),
      col = col_pal[i]
    )
  }
}

plot_pdf_cdf(points, pdfs, cdfs)
```

### 2. Generate n = 100 random numbers from each distribution.

```{r generate-distributions}
n <- 100

generate_samples <- function(n) {
  samples <- list()

  # Binomial distribution
  samples$Binomial <- rbinom(n, size = N, prob = p)

  # Geometric distribution
  samples$Geometric <- rgeom(n, prob = p)

  # Poisson distribution
  samples$Poisson <- rpois(n, lambda = lambda)

  # Continuous uniform distribution
  samples$Uniform <- runif(n, min = 0, max = 1)

  # Exponential distribution
  samples$Exponential <- rexp(n, rate = rate)

  # Normal distribution
  samples$Normal <- rnorm(n, mean = mean, sd = sd)

  # Student's t distribution
  samples$Student <- rt(n, df = df)

  # Gamma distribution
  samples$Gamma <- rgamma(n, shape = shape, scale = scale)

  return(samples)
}

samples <- generate_samples(n)
```

(a) Plot histograms of the relative frequency distribution of your random sample using appropriate bin widths.

(b) Overlay the true probability density function. Do they match?

```{r plot-histograms}
plot_distributions <- function(points, samples, pdfs) {
  n <- length(samples)
  col_pal <- rainbow(n)
  # par(mfrow = c(ceiling(sqrt(n / 2)), 2 * ceiling(sqrt(n / 2))))
  par(mfrow = c(1, 1))

  for (i in 1:n) {
    m <- length(samples[[i]])
    hist(samples[[i]],
      freq = FALSE,
      xlab = "x",
      ylab = "Density",
      main = paste(names(samples)[i], " distribution (n = ", m, ")", sep = ""),
      col = col_pal[i],
      border = "white"
    )
    lines(points, pdfs[[i]],
      lwd = 2,
      col = "red"
    )
  }
}

plot_distributions(points, samples, pdfs)
```

(c) Analytically calculate the theoretical mean and variance given your choice of parameters.

```{r theoretical-moments}
print_mean_var <- function(name, mean, var) {
  print(paste(name, "distribution: mean =", mean))
  print(paste(name, "distribution: variance =", var))
}

# Binomial distribution
print_mean_var("Binomial", N * p, N * p * (1 - p))
# Geometric distribution
print_mean_var("Geometric", 1 / p, (1 - p) / p^2)
# Poisson distribution
print_mean_var("Poisson", lambda, lambda)
# Continuous uniform distribution
print_mean_var("Uniform", 0.5, 1 / 12)
# Exponential distribution
print_mean_var("Exponential", 1 / rate, 1 / rate^2)
# Normal distribution
print_mean_var("Normal", mean, sd^2)
# Student's t distribution
print_mean_var("Student", 0, df / (df - 2))
# Gamma distribution
print_mean_var("Gamma", shape * scale, shape * scale^2)
```

(d) Calculate the sample mean (mean) and variance (var) for your random sample. Do they match the theoretical quantities?

```{r sample-moments}
for (i in seq_along(samples)) {
  print_mean_var(names(samples)[i], mean(samples[[i]]), var(samples[[i]]))
}
```

(e) Repeat (a-d) using n = 200, 500, 1000. What happens as the sample size increases?

```{r increase-sample-size}
n_values <- c(250, 500, 1000)

for (n in n_values) {
  samples <- generate_samples(n)
  plot_distributions(points, samples, pdfs)

  for (i in seq_along(samples)) {
    print_mean_var(names(samples)[i], mean(samples[[i]]), var(samples[[i]]))
  }
}
```
